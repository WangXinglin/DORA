# refer to src/sal/config.py for more options
approach: dora
n: 64
search_batch_size: 1
seed: 0
prm_batch_size: 2
max_tokens: 2048
max_tokens_per_step: 256
temperature: 0.8
balance_alpha: 0.01
output_dir: "/mnt/public/usr/yourpath/search-and-learn/result/MATH500/Qwen2.5-Math-PRM-7B/Llama-3.2-1B-Instruct/dora"
prm_path: "/mnt/public/usr/yourpath/allmodels/Qwen2.5-Math-PRM-7B"
embedding_path: "/mnt/public/usr/yourpath/allmodels/bge-m3"